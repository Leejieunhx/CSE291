# -*- coding: utf-8 -*-
"""modified_baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X8-XFpZoRQwLyUl1y88bOdp4W4W5labr
"""

import os
import time
start_time_training = time.time()

def print_cpu_times():
    cpu_times = os.times()
    print(f"User time: {cpu_times.user} seconds")
    print(f"System time: {cpu_times.system} seconds")
    print(f"Total CPU time: {cpu_times.user + cpu_times.system} seconds")

import pickle
import pandas as pd
import itertools
from collections import Counter
import numpy as np
from nltk import word_tokenize
from nltk.corpus import stopwords
from gensim.models import word2vec
from sklearn.linear_model import LogisticRegression
import os
import string

import nltk
nltk.download('stopwords')
nltk.download('punkt')

def build_vocab(sentences):
    # Build vocabulary
    word_counts = Counter(itertools.chain(*sentences))
    # Mapping from index to word
    vocabulary_inv = [x[0] for x in word_counts.most_common()]
    # Mapping from word to index
    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}
    return word_counts, vocabulary, vocabulary_inv

# Start timing for preprocessing
start_time_preprocess = time.time()

# Existing preprocessing code
def get_embeddings(inp_data, vocabulary_inv, size_features=100,
                   mode='skipgram',
                   min_word_count=2,
                   context=5):
    model_name = "embedding"
    model_name = os.path.join(model_name)
    num_workers = 15  # Number of threads to run in parallel
    downsampling = 1e-3  # Downsample setting for frequent words
    print('Training Word2Vec model...')
    sentences = [[vocabulary_inv[w] for w in s] for s in inp_data]
    if mode == 'skipgram':
        sg = 1
        print('Model: skip-gram')
    elif mode == 'cbow':
        sg = 0
        print('Model: CBOW')
    embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,
                                        sg=sg,
                                        vector_size=size_features,
                                        min_count=min_word_count,
                                        window=context,
                                        sample=downsampling)
    embedding_model.init_sims(replace=True)
    print("Saving Word2Vec model {}".format(model_name))
    embedding_weights = np.zeros((len(vocabulary_inv), embedding_model.vector_size))
    for i, word in enumerate(vocabulary_inv):
        if word in embedding_model.wv:  # Use .wv to access the word vectors
            embedding_weights[i] = embedding_model.wv[word]
        else:
            embedding_weights[i] = np.random.normal(scale=0.6, size=(embedding_model.vector_size,))
    return embedding_weights

# End timing for preprocessing
end_time_preprocess = time.time()

# Print CPU times for preprocessing
print_cpu_times()

# Calculate and print wall time for preprocessing
wall_time_preprocess = end_time_preprocess - start_time_preprocess
print(f"Wall time for preprocessing: {wall_time_preprocess} seconds")

# Existing training and prediction code
def preprocess_df(df):
    stop_words = set(stopwords.words('english'))
    stop_words.add('would')
    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))
    preprocessed_sentences = []
    for i, row in df.iterrows():
        sent = row["text"]
        sent_nopuncts = sent.translate(translator)
        words_list = sent_nopuncts.strip().split()
        filtered_words = [word for word in words_list if word not in stop_words and len(word) != 1]
        preprocessed_sentences.append(" ".join(filtered_words))
    df["text"] = preprocessed_sentences
    return df

df_train = pd.read_csv("train.csv", on_bad_lines='skip', engine='python')
df_test = pd.read_csv("test.csv", on_bad_lines='skip', engine='python')

df_train["text"] = df_train["review"]
df_test["text"] = df_test["review"]
df_train = preprocess_df(df_train)

df_test = preprocess_df(df_test)

tagged_data = [word_tokenize(_d) for i, _d in enumerate(df_train["text"])]
word_counts, vocabulary, vocabulary_inv = build_vocab(tagged_data)
inp_data = [[vocabulary[word] for word in text] for text in tagged_data]
embedding_weights = get_embeddings(inp_data, vocabulary_inv)


tagged_train_data = [word_tokenize(_d) for i, _d in enumerate(df_train["text"])]
tagged_test_data = [word_tokenize(_d) for i, _d in enumerate(df_test["text"])]

train_vec = []
for doc in tagged_train_data:
    vec = 0
    for w in doc:
        vec += embedding_weights[vocabulary[w]]
    vec = vec / len(doc)
    train_vec.append(vec)

test_vec = []
for doc in tagged_test_data:
    vec = 0
    length = 0
    for w in doc:
        try:
            vec += embedding_weights[vocabulary[w]]
            length += 1
        except:
            continue
    vec = vec / length
    test_vec.append(vec)

clf = LogisticRegression(max_iter=100000000).fit(train_vec, df_train["label"])
preds = clf.predict(test_vec)

dic = {"Id": [], "Predicted": []}
for i, pred in enumerate(preds):
    dic["Id"].append(i)
    dic["Predicted"].append(pred)

dic_df = pd.DataFrame.from_dict(dic)
dic_df.to_csv("predicted.csv", index=False)

# End timing for training
end_time_training = time.time()

# Print CPU times for training
print_cpu_times()

# Calculate and print wall time for training
wall_time_training = end_time_training - start_time_training
print(f"Wall time for training: {wall_time_training} seconds")